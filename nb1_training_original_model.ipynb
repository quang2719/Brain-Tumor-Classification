{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. This notebook is my original work on the brain tumor classification task. It has undergone many changes based on my teacher's recommendations. The first version suffered from overfitting, which I addressed using various methods in the nb2_optimization_model notebook. You can check it for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. You can test my model using the nb0_predict_new_img notebook to test with new images in the pred folder. Detailed instructions are provided in that notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from keras.utils import normalize\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoid difference result after rerun notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 99\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dir = os.listdir('./data_no/data_no/NO/')\n",
    "yes_dir = os.listdir('./data_yes/data_yes/YES/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "535"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(no_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "557"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(yes_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set,label = [],[]\n",
    "for i,cur_img_dir in enumerate(no_dir):\n",
    "    #check type of image\n",
    "    if cur_img_dir.split('.')[1]=='jpg':\n",
    "        img = cv2.imread('./data_no/data_no/NO/'+cur_img_dir)\n",
    "        img = Image.fromarray(img,'RGB')\n",
    "        img = img.resize((64,64))\n",
    "        data_set.append(np.array(img))\n",
    "        label.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,cur_img_dir in enumerate(yes_dir):\n",
    "    #check type of image\n",
    "    if cur_img_dir.split('.')[1]=='jpg':\n",
    "        img = cv2.imread('./data_yes/data_yes/YES/'+cur_img_dir)\n",
    "        img = Image.fromarray(img,'RGB')\n",
    "        img = img.resize((64,64))\n",
    "        data_set.append(np.array(img))\n",
    "        label.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1092, 64, 64, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set = np.array(data_set)\n",
    "label = np.array(label)\n",
    "data_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1092,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes observe:557, no observe:535\n"
     ]
    }
   ],
   "source": [
    "print(f'yes observe:{sum(label)}, no observe:{len(label)-sum(label)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split and normalize data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(\n",
    "    data_set,label,\n",
    "    test_size=0.2,\n",
    "    random_state=9\n",
    "    )\n",
    "x_train,x_val,y_train,y_val = train_test_split(\n",
    "        x_train,y_train,\n",
    "    test_size=0.25,\n",
    "    random_state=9\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape: (654, 64, 64, 3)\n",
      "Y train shape: (654,)\n",
      "X test shape: (219, 64, 64, 3)\n",
      "Y test shape: (219,)\n",
      "X validation shape: (219, 64, 64, 3)\n",
      "Y validation shape: (219, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f'X train shape: {x_train.shape}\\nY train shape: {y_train.shape}\\nX test shape: {x_test.shape}\\nY test shape: {y_test.shape}\\nX validation shape: {x_val.shape}\\nY validation shape: {x_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import joblib \n",
    "# using scaler of nb4\n",
    "scaler = joblib.load('scaler.pkl') \n",
    "# Reshape data to fit with MinMaxScaler\n",
    "x_train_reshaped = x_train.reshape(-1, x_train.shape[-1])\n",
    "x_test_reshaped = x_test.reshape(-1, x_test.shape[-1])\n",
    "x_val_reshaped = x_val.reshape(-1, x_val.shape[-1])\n",
    "\n",
    "x_train_reshaped = scaler.fit_transform(x_train_reshaped)\n",
    "x_test_reshaped = scaler.transform(x_test_reshaped)\n",
    "x_val_reshaped = scaler.transform(x_val_reshaped)\n",
    "\n",
    "# Reshape to original shape\n",
    "x_train = x_train_reshaped.reshape(x_train.shape)\n",
    "x_test = x_test_reshaped.reshape(x_test.shape)\n",
    "x_val = x_val_reshaped.reshape(x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import (\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Activation,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    Dense\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "INPUT_SIZE = 64\n",
    "\n",
    "model.add(Conv2D(32,(3,3), input_shape=(INPUT_SIZE,INPUT_SIZE,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(32, (3,3), kernel_initializer='he_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64,(3,3), kernel_initializer='he_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*update 25/08 - adding early stopping method* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - accuracy: 0.6539 - loss: 0.6269 - val_accuracy: 0.7626 - val_loss: 0.4704\n",
      "Epoch 2/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.8628 - loss: 0.3625 - val_accuracy: 0.8402 - val_loss: 0.3854\n",
      "Epoch 3/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9235 - loss: 0.2331 - val_accuracy: 0.8584 - val_loss: 0.3119\n",
      "Epoch 4/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - accuracy: 0.9497 - loss: 0.1672 - val_accuracy: 0.8767 - val_loss: 0.2972\n",
      "Epoch 5/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9528 - loss: 0.1531 - val_accuracy: 0.8447 - val_loss: 0.3211\n",
      "Epoch 6/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9605 - loss: 0.1417 - val_accuracy: 0.8539 - val_loss: 0.3394\n",
      "Epoch 7/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.9574 - loss: 0.1183 - val_accuracy: 0.8995 - val_loss: 0.2369\n",
      "Epoch 8/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.9712 - loss: 0.0868 - val_accuracy: 0.8858 - val_loss: 0.2586\n",
      "Epoch 9/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9821 - loss: 0.0578 - val_accuracy: 0.8311 - val_loss: 0.3663\n",
      "Epoch 10/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.9757 - loss: 0.0652 - val_accuracy: 0.9406 - val_loss: 0.1982\n",
      "Epoch 11/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9853 - loss: 0.0359 - val_accuracy: 0.8676 - val_loss: 0.3001\n",
      "Epoch 12/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.9860 - loss: 0.0521 - val_accuracy: 0.9406 - val_loss: 0.1619\n",
      "Epoch 13/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.9899 - loss: 0.0258 - val_accuracy: 0.9315 - val_loss: 0.1960\n",
      "Epoch 14/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9861 - loss: 0.0374 - val_accuracy: 0.9361 - val_loss: 0.1965\n",
      "Epoch 15/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9964 - loss: 0.0254 - val_accuracy: 0.9452 - val_loss: 0.1628\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Compile the model with accuracy metric\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(amsgrad=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Define EarlyStopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=3,          # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=16,\n",
    "    epochs=50,\n",
    "    validation_data=(x_val, y_val),\n",
    "    shuffle=False,  # Determine whether to shuffle the training data before each epoch\n",
    "    callbacks=[early_stopping]  # Add EarlyStopping callback\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step\n",
      "(219, 1)\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model.predict(x_test)\n",
    "print(y_test_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(219, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = (y_test_pred >0.5).astype(int)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(219, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_reshape = y_test.reshape(-1,1)\n",
    "y_test_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in test set: 0.958904109589041\n",
      "Accuracy in validation set: 0.9452054500579834\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy in test set:\", accuracy_score(y_test_reshape, y_pred))\n",
    "print('Accuracy in validation set:',history.history['val_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save original model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Infor**<br>\n",
    "Accuracy in test set: 0.958904109589041<br>\n",
    "Accuracy in validation set: 0.9452054500579834<br>\n",
    "Name: BrainTurmor_v2<br>\n",
    "\n",
    "Status: Good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save original model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('BrainTurmor_v1.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
