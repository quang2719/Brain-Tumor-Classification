{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning, finetune VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG16 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford in the paper \"Very Deep Convolutional Networks for Large-Scale Image Recognition\". It is one of the most popular pre-trained models for image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Features of VGG16\n",
    "1. Architecture: VGG16 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford in the paper \"Very Deep Convolutional Networks for Large-Scale Image Recognition\".<br>\n",
    "2. Layers: It has 16 layers with learnable weights, including 13 convolutional layers and 3 fully connected layers.<br>\n",
    "3. Pre-trained Weights: The model is pre-trained on the ImageNet dataset, which contains 1.2 million images and 1000 classes.<br>\n",
    "4. Usage: It is widely used for image classification tasks and can be fine-tuned for specific tasks such as brain tumor classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Finetune VGG16 for brain tumor classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from keras.utils import normalize\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 99\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading and nomalize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dir = os.listdir('./data_no/data_no/NO/')\n",
    "yes_dir = os.listdir('./data_yes/data_yes/YES/')\n",
    "data_set,label = [],[]\n",
    "for i,cur_img_dir in enumerate(no_dir):\n",
    "    #check type of image\n",
    "    if cur_img_dir.split('.')[1]=='jpg':\n",
    "        img = cv2.imread('./data_no/data_no/NO/'+cur_img_dir)\n",
    "        img = Image.fromarray(img,'RGB')\n",
    "        img = img.resize((64,64))\n",
    "        data_set.append(np.array(img))\n",
    "        label.append(0)\n",
    "for i,cur_img_dir in enumerate(yes_dir):\n",
    "    #check type of image\n",
    "    if cur_img_dir.split('.')[1]=='jpg':\n",
    "        img = cv2.imread('./data_yes/data_yes/YES/'+cur_img_dir)\n",
    "        img = Image.fromarray(img,'RGB')\n",
    "        img = img.resize((64,64))\n",
    "        data_set.append(np.array(img))\n",
    "        label.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = np.array(data_set)\n",
    "label = np.array(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(\n",
    "    data_set,label,\n",
    "    test_size=0.2,\n",
    "    random_state=99\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **read k fold cross validation**\n",
    "### **using pytorch**\n",
    "**run resnet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape: (873, 64, 64, 3)\n",
      "Y train shape: (873,)\n",
      "X test shape: (219, 64, 64, 3)\n",
      "Y test shape: (219,))\n"
     ]
    }
   ],
   "source": [
    "print(f'X train shape: {x_train.shape}\\nY train shape: {y_train.shape}\\nX test shape: {x_test.shape}\\nY test shape: {y_test.shape})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nomalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = normalize(x_train, axis=1)\n",
    "x_test = normalize(x_test, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VGG16 model with pre-trained weights, excluding the top fully connected layers\n",
    "base_model = VGG16(\n",
    "    weights='imagenet', # bo hoac none\n",
    "    include_top=False, \n",
    "    input_shape=(64, 64, 3)\n",
    "    )\n",
    "\n",
    "# Freeze the convolutional base\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False # True for fine tune model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom top layers for classification\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dense(1, activation='sigmoid')(x)  #binary classification \n",
    "# note 1 dense +1 activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new model\n",
    "model = Model(inputs=base_model.input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 64, 64, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 64, 64, 64)        1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 64, 64, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 16, 16, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 8, 8, 512)         1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 512)               1049088   \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15764289 (60.14 MB)\n",
      "Trainable params: 1049601 (4.00 MB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "28/28 [==============================] - 14s 464ms/step - loss: 0.6050 - accuracy: 0.6964 - val_loss: 0.4355 - val_accuracy: 0.7854\n",
      "Epoch 2/10\n",
      "28/28 [==============================] - 13s 477ms/step - loss: 0.4566 - accuracy: 0.7950 - val_loss: 0.4353 - val_accuracy: 0.8082\n",
      "Epoch 3/10\n",
      "28/28 [==============================] - 14s 496ms/step - loss: 0.4193 - accuracy: 0.8041 - val_loss: 0.3738 - val_accuracy: 0.8219\n",
      "Epoch 4/10\n",
      "28/28 [==============================] - 14s 500ms/step - loss: 0.3526 - accuracy: 0.8511 - val_loss: 0.3184 - val_accuracy: 0.8676\n",
      "Epoch 5/10\n",
      "28/28 [==============================] - 14s 511ms/step - loss: 0.3228 - accuracy: 0.8740 - val_loss: 0.2900 - val_accuracy: 0.8950\n",
      "Epoch 6/10\n",
      "28/28 [==============================] - 14s 506ms/step - loss: 0.2991 - accuracy: 0.8774 - val_loss: 0.2689 - val_accuracy: 0.9041\n",
      "Epoch 7/10\n",
      "28/28 [==============================] - 15s 526ms/step - loss: 0.2731 - accuracy: 0.8820 - val_loss: 0.2561 - val_accuracy: 0.9041\n",
      "Epoch 8/10\n",
      "28/28 [==============================] - 14s 515ms/step - loss: 0.2683 - accuracy: 0.8946 - val_loss: 0.2344 - val_accuracy: 0.9087\n",
      "Epoch 9/10\n",
      "28/28 [==============================] - 15s 521ms/step - loss: 0.2644 - accuracy: 0.8889 - val_loss: 0.2474 - val_accuracy: 0.8995\n",
      "Epoch 10/10\n",
      "28/28 [==============================] - 14s 516ms/step - loss: 0.2246 - accuracy: 0.9118 - val_loss: 0.2217 - val_accuracy: 0.9269\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=Adam(lr=1e-4, amsgrad=True),\n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=20, \n",
    "    batch_size=32, \n",
    "    validation_data = (x_test, y_test)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
